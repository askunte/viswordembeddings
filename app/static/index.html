<!DOCTYPE html>
<html>
<head>
    <script src="https://code.jquery.com/jquery-3.2.1.min.js"></script>
    <link rel="stylesheet" type="text/css" href="/s/style.css">
    <body>
        <div id="navbar">
          <img class="logo" src="http://pages.cs.wisc.edu/~heimerl/crest.svg"></img>
          <a href="/", id="about", class="navactive">About</a>
          <a href="/s/neighborhoods.html", id="neighbors", class="navinactive">Neighborhoods</a>
          <a href="/s/scatter.html", id="projections", class="navinactive">Concept Projection</a>
          <a href="/s/coocs.html", id="coocs", class="navinactive">Co-occurrences</a>
        </div>

        <div id="main">
            <div class="textblock">
                <h1>What is this page about?</h1>
                <p>
                This page contains interactive designs for exploring and analyzing word vector embeddings.
                For more information about the implementations and the ideas behind it, please refer to our <a href="http://pages.cs.wisc.edu/~heimerl/files/poster_summary.pdf" target="_blank">paper</a> presented at <a href="https://www.eurovis2018.org" target="_blank">EuroVis 2018</a>.
                If you use our implementations or our code for your own project, please cite us as follows:
                </p>
                <pre>
@Article{HG18,
  author       = {Heimerl, Florian and Gleicher, Michael},
  title        = {Interactive Analysis of Word Vector Embeddings},
  journal      = {Computer Graphics Forum},
  number       = {3},
  volume       = {37},
  month        = {jun},
  year         = {2018},
  projecturl   = {http://graphics.cs.wisc.edu/Vis/EmbVis},
  url          = {http://graphics.cs.wisc.edu/Papers/2018/HG18}
}
                </pre>
                <p>
                We provide 22 example embeddings in the right format, ready to use and explore with our implementations:
                    <ul>
                        <li><b>1800-1990:</b> The
                            <a href="https://github.com/williamleif/histwords" target="_blank">HistWords</a>
                            embeddings created by
                            <a href="https://web.stanford.edu/~jurafsky/pubs/paper-hist_vec.pdf" target="_blank">Hamilton et al.</a>
                            We use their "All English" data set.
                            Those embeddings are trained on Google n-grams for each decade from 1800-1990, resulting in 20 embeddings
                            (<a href="https://pages.cs.wisc.edu/~heimerl/hamilton_embeddings.tar.bz2">download</a>).
                        </li>
                        <li><b>EEBO_TCP:</b>
                            Word embedding trained with
                            <a href="https://nlp.stanford.edu/projects/glove/" target="_blank">GloVe</a>
                            on the historic
                            <a href="http://www.textcreationpartnership.org/tcp-eebo/" target="_blank">EEBO-TCP</a>
                            corpus.
                            This embeddings is trained with a window size of 15, and a minimum word count of 5
                            (<a href="https://pages.cs.wisc.edu/~heimerl/eebo_embeddings.tar.bz2">download</a>).
                        </li>
                        <li><b>wiki1-5:</b>
                            Word embeddings trained with
                            <a href="https://nlp.stanford.edu/projects/glove/" target="_blank">GloVe</a>
                            on the entire English
                            <a href="http://www.wikipedia.org" target="_blank">Wikipedia</a>.
                            All five embeddings are trained with a window size of 15, and a minimum word count of 5
                            (<a href="https://pages.cs.wisc.edu/~heimerl/wiki_embeddings.tar.bz2">download</a>).
                        </li>
                    </ul>
                </p>
                <h1>Cool! Can I use this for my own data?</h1>
                <p>
                If you are interested in analyzing your own embeddings, you can either directly work with our code available on <a href="https://github.com/uwgraphics/viswordembeddings" target="_blank">GitHub</a>.
                Alternatively, a <a href="https://hub.docker.com/r/fheimerl/viswordembeddings/" target="_blank">docker image</a> that runs the implementations on a web server is available.
                More information in how to set up and run the image is available <a href="http://graphics.cs.wisc.edu/Vis/EmbVis/" target="_blank">here</a>.
                </p>
            </div>
        </div>
    </body>
</html>
